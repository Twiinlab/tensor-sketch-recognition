{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savefile = \"./STORED_model/my_trained_model.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to help intialize random weights for fully connected or convolutional layers, we leave the shape attribute as a parameter for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as init_weights, but for the biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D convolution using builtin conv2d from TF. From those docs:\n",
    "\n",
    "Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n",
    "\n",
    "Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
    "and a filter / kernel tensor of shape\n",
    "`[filter_height, filter_width, in_channels, out_channels]`, this op\n",
    "performs the following:\n",
    "\n",
    "1. Flattens the filter to a 2-D matrix with shape\n",
    "   `[filter_height * filter_width * in_channels, output_channels]`.\n",
    "2. Extracts image patches from the input tensor to form a *virtual*\n",
    "   tensor of shape `[batch, out_height, out_width,\n",
    "   filter_height * filter_width * in_channels]`.\n",
    "3. For each patch, right-multiplies the filter matrix and the image patch\n",
    "   vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a max pooling layer, again using built in TF functions:\n",
    "\n",
    "Performs the max pooling on the input.\n",
    "\n",
    "    Args:\n",
    "      value: A 4-D `Tensor` with shape `[batch, height, width, channels]` and\n",
    "        type `tf.float32`.\n",
    "      ksize: A list of ints that has length >= 4.  The size of the window for\n",
    "        each dimension of the input tensor.\n",
    "      strides: A list of ints that has length >= 4.  The stride of the sliding\n",
    "        window for each dimension of the input tensor.\n",
    "      padding: A string, either `'VALID'` or `'SAME'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the conv2d function, we'll return an actual convolutional layer here that uses an ReLu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a normal fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper (custom functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(pos):\n",
    "    '''\n",
    "    For use to one-hot encode the 10- possible labels\n",
    "    '''\n",
    "    out = np.zeros(10)\n",
    "    out[pos] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duck lenght 135480\n",
      "smile lenght 124386\n",
      "car lenght 182764\n",
      "pencil lenght 122001\n",
      "star lenght 137619\n",
      "burger lenght 129672\n",
      "cookie lenght 131353\n",
      "rabbit lenght 155288\n",
      "moon lenght 121661\n",
      "icecream lenght 123133\n"
     ]
    }
   ],
   "source": [
    "#duck smile car pencil star burger cookie rabbit moon icecream\n",
    "fileList = ['duck','smile','car','pencil','star','burger','cookie','rabbit','moon','icecream']\n",
    "for i in range(len(fileList)):\n",
    "    print('{} lenght {}'.format(fileList[i], len(np.load('./SKETCH_data/'+fileList[i]+'.npy'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images = []\n",
    "# pos_begin = 0\n",
    "# pos_end = 100\n",
    "# fileList = ['duck','smile','car','pencil','star','burger','cookie','rabbit','moon','icecream']\n",
    "# images = np.array(np.load('./SKETCH_data/'+ fileList[0] +'.npy')[pos_begin:pos_end])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.concatenate((images,np.array(np.load('./SKETCH_data/'+ fileList[1] +'.npy')[pos_begin:pos_end])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(img, label, predict):\n",
    "    plt.title('Real %s. Predict: %s - %s' % (label, predict, \"Correct\" if (label==predict) else \"No correct\" ))\n",
    "    plt.imshow(img.reshape((28,28)), cmap=plt.cm.gray_r)\n",
    "    plt.show()\n",
    "    \n",
    "# usage: display(test_x[0], 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SketchImageHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Init SketchImageHelper\")\n",
    "        self.position = 0\n",
    "        \n",
    "        self.batch_x = None\n",
    "        self.batch_y = None\n",
    "        \n",
    "        self.pos_begin = 1000\n",
    "        self.pos_end = 110000\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.fileList = ['duck','smile','car','pencil','star','burger','cookie','rabbit','moon','icecream']\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Batch Images and Labels\")\n",
    "        sampleSize = self.pos_end - self.pos_begin\n",
    "        i = 0\n",
    "        for i in range(len(self.fileList)):\n",
    "            partialImages = np.array(np.load('./SKETCH_data/'+ self.fileList[i] +'.npy')[self.pos_begin:self.pos_end])\n",
    "            self.images.append( partialImages / 255)\n",
    "            self.labels.append(np.full((sampleSize,10), one_hot_encode(i)))\n",
    "\n",
    "        print('batch lenght {}'.format(len(self.images)))\n",
    "        print('batch lenght {}'.format(len(self.labels)))\n",
    "        \n",
    "        \n",
    "    def next_batch(self, batch_size):                          \n",
    "        x = []\n",
    "        y = []\n",
    "        partial_batch = batch_size // len(self.fileList)\n",
    "        i = 0\n",
    "        for i in range(len(self.fileList)):\n",
    "            if i==0:\n",
    "                x = np.array((self.images[i])[self.position:self.position+partial_batch])\n",
    "                y = np.array((self.labels[i])[self.position:self.position+partial_batch])\n",
    "            else:\n",
    "                x = np.concatenate((x,np.array((self.images[i])[self.position:self.position+partial_batch])), axis=0)\n",
    "                y = np.concatenate((y,np.array((self.labels[i])[self.position:self.position+partial_batch])), axis=0)  \n",
    "\n",
    "        \n",
    "        self.position = (self.position + partial_batch)\n",
    "        print(' {}'.format(self.position), end='')\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sih = SketchImageHelper()\n",
    "# sih.set_up_images()\n",
    "# lotx, loty = sih.next_batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(sih.images[0]))\n",
    "# display(sih.images[0][51], 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(lotx)\n",
    "# display(lotx[4], 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lotx, loty = sih.next_batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(lotx)\n",
    "# display(lotx[4], 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lotx, loty = sih.next_batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a 6by6 filter here, used 5by5 in video, you can play around with the filter size\n",
    "# You can change the 32 output, that essentially represents the amount of filters used\n",
    "# You need to pass in 32 to the next input though, the 1 comes from the original input of \n",
    "# a single image.\n",
    "convo_1 = convolutional_layer(x_image,shape=[6,6,1,32])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a 6by6 filter here, used 5by5 in video, you can play around with the filter size\n",
    "# You can actually change the 64 output if you want, you can think of that as a representation\n",
    "# of the amount of 6by6 filters used.\n",
    "convo_2 = convolutional_layer(convo_1_pooling,shape=[6,6,32,64])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Why 7 by 7 image? Because we did 2 pooling layers, so (28/2)/2 = 7\n",
    "# 64 then just comes from the output of the previous Convolution\n",
    "convo_2_flat = tf.reshape(convo_2_pooling,[-1,7*7*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE THE PLACEHOLDER HERE!\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001) # 0.0001\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_x lenght 5000\n",
      "test_y lenght 5000\n"
     ]
    }
   ],
   "source": [
    "#duck smile car pencil star burger cookie rabbit moon icecream\n",
    "\n",
    "pos_begin = 0\n",
    "pos_end = 500\n",
    "\n",
    "test_x = np.concatenate((np.array(np.load('./SKETCH_data/duck.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/smile.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/car.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/pencil.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/star.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/burger.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/cookie.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/rabbit.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/moon.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/icecream.npy')[pos_begin:pos_end])), axis=0)\n",
    "test_y = np.concatenate((np.full((pos_end-pos_begin,10), one_hot_encode(0)), np.full((pos_end-pos_begin,10), one_hot_encode(1)), np.full((pos_end-pos_begin,10), one_hot_encode(2)), np.full((pos_end-pos_begin,10), one_hot_encode(3)),\n",
    "                               np.full((pos_end-pos_begin,10), one_hot_encode(4)), np.full((pos_end-pos_begin,10), one_hot_encode(5)), np.full((pos_end-pos_begin,10), one_hot_encode(6)), np.full((pos_end-pos_begin,10), one_hot_encode(7)),\n",
    "                               np.full((pos_end-pos_begin,10), one_hot_encode(8)), np.full((pos_end-pos_begin,10), one_hot_encode(9))), axis=0)\n",
    "\n",
    "print('test_x lenght {}'.format(len(test_x)))\n",
    "print('test_y lenght {}'.format(len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sih = SketchImageHelper()\n",
    "# sih.set_up_images()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "    \n",
    "#     sess.run(init)\n",
    "#     batch_x , batch_y = sih.next_batch(500)\n",
    "    \n",
    "#     print(batch_x[0])\n",
    "#     print(batch_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init SketchImageHelper\n",
      "Setting Up Batch Images and Labels\n",
      "batch lenght 10\n",
      "batch lenght 10\n"
     ]
    }
   ],
   "source": [
    "sih = SketchImageHelper()\n",
    "sih.set_up_images()\n",
    "\n",
    "#sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT\n",
      " 105050\n",
      "\n",
      "step 0\n",
      "Accuracy is:\n",
      "0.1308\n",
      " 105100 105150 105200 105250 105300 105350 105400 105450 105500 105550 105600 105650 105700 105750 105800 105850 105900 105950 106000 106050 106100 106150 106200 106250 106300 106350 106400 106450 106500 106550 106600 106650 106700 106750 106800 106850 106900 106950 107000 107050 107100 107150 107200 107250 107300 107350 107400 107450 107500 107550\n",
      "\n",
      "step 50\n",
      "Accuracy is:\n",
      "0.5476\n",
      " 107600 107650 107700 107750 107800 107850 107900 107950 108000 108050 108100 108150 108200 108250 108300 108350 108400 108450 108500 108550 108600 108650 108700 108750 108800 108850 108900 108950 109000 109050 109100 109150 109200 109250 109300 109350 109400 109450 109500 109550 109600 109650 109700 109750 109800 109850 109900 109950 110000\n",
      "\n",
      "FINAL Accuracy is:\n",
      "0.5732\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    steps = 100\n",
    "\n",
    "    print('INIT')\n",
    "    sess.run(init)\n",
    "\n",
    "    for j in range(steps):\n",
    "        # print('.', end='')\n",
    "        batch_x , batch_y = sih.next_batch(500)\n",
    "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5})\n",
    "\n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if j%50 == 0:\n",
    "            print('\\n')\n",
    "            print('step {}'.format(j))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={x:test_x,y_true:test_y,hold_prob:1.0}))\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('FINAL Accuracy is:')\n",
    "    print(sess.run(acc,feed_dict={x:test_x,y_true:test_y,hold_prob:1.0}))\n",
    "    print('\\n')\n",
    "    \n",
    "    tf.train.Saver().save(sess, savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./STORED_model/my_trained_model.json\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFFVJREFUeJzt3XuwlPV9x/H3RxIvgPUCp4hcBOMFLTOgOUElGS+xMepM\nB52ORE0sYRKJibXNjJNqYmKcNmNtasS0yWhQiZKJWG+p1Bo7qFVk4miODgjqxAseC5TLQTGgaFT4\n9o99Troez/52OWdv8Pu8Zs6c3ee7zz7f/e1+9vI8u8+jiMDM8rNHqxsws9Zw+M0y5fCbZcrhN8uU\nw2+WKYffLFNZhV/So5K+WuNlT5a0ppnLbCZJX5a0tOz8W5IObcJyd+tx3ZW0XfgldUt6p3gwrpd0\nq6Thre6r2SRNkBTFOLxVjMvljVpeRAyPiFU19vSxRvXRKpKmSXpA0puS3pD0lKTZbdBXw8a87cJf\n+IuIGA5MBY4Bvt3iflpp/2IszgOulHR63wvsjmFsJkknAI8AjwGHASOArwMfGesarusj90W73j/t\nGn4AImI98F+UngQAkLSXpGsl/Y+kDZJulLRPUTtA0v2SeiRtLk6PrWVZkvYp3mVslvQ88Kk+9ZB0\nWNn5WyX9oOz8DEnLJG2R9EqFkI6W9Kykbw1gLJ4AngMml/VzsaSXgJeKaZMkLS5euX4naWbZskdI\nWlT09xTwiUq3rxiLH0l6TdLvJS0txnhJcfE3i3cjJ1Tru93HtfDPwG0R8U8RsSlKno6IL5Qt40JJ\nLxdju0jSwX1uQ9/7Ymfvn7qNec0ioq3+gG7gz4vTY4EVwI/L6nOBRcCBwL7AfwD/WNRGAH8JDC1q\ndwH/Xjbvo8BXKyz3GuDx4nrHASuBNWX1AA4rO38r8IPi9DTg98DnKD2hjgEmlS8TmAi8CMypcRwm\nFMv8GCDg08A24NSyfhYX/e4DDANWA7OLeY4BNgFHF5e/A7izuNxkYC2wtL/bB/y06HsMMASYDuxV\n3lPZfOOBN4Hxu8K49tPfUGA7cEriMp8txvLYYhz+FVjS5zb88b4Y4P1T85jXLWutDnuF8L8FbC1u\n9MOU3vpShOBt4BNllz8BeLXCdU0FNtcY/lXA6WXn5+zEg/RnwNwK1/socF1xu87biXHovdPfBDYD\nLwB/06efz5ad/wLweJ/r+Bnw/eLB9H5vcIra1fQT/iJk7wBTEj3V/EBst3Ht53rGFD1MSlzmFuCH\nZeeHF+M5ob/7YgD3T13HvNa/dn3bf1ZE7AucDEwCRhbTOyg9Uz9drJh5E3iwmI6koZJ+Vrx12kLp\nLdP+kobUsMyDKT0z93ptJ/odB7ySqH+R0ivt3Ttxnb1GRsQBEXFURPxLn1p5v4cAx/WOSzE2XwQO\nojQ+H6O22zcS2Jv07dkZLR1XSV8sW2n6634ushnYAYxOLOdgyvqOiLeA1yk9cfRa3Xcmar9/6j3m\nNWnX8AMQEY9ReiW4tpi0idIz5J9FxP7F335RWiEGcClwJHBcRPwJcGIxXTUsbh2lB1uv8X3q2yg9\n8fQ6qOz0avp8hu7jqqL322t8IqpV+U8yVwOPlY3L/lFag/91oAf4gPTt67UJeJf+b89AfgLa0nGN\niF8W4zA8Is7op74NeILSx8VK/pdSeAGQNIzSR8y15VfV3+LLTqfun3qPeU3aOvyF64HPSZoSETuA\nm4C5kv4UQNIYSZ8vLrsvpSeHNyUdSOktVa3uBL5drDQcC1zSp74MOF/SkGKl00lltVuA2ZJOlbRH\n0dOksvr7wDmUPvctkNSIcb8fOELSBZI+Xvx9StJREbEduBe4qnh3dDQwq78rKcZ4PnCdpIOL23uC\npL0oPYnsAHbm+wC7wrj+HfBlSd+SNAJA0hRJdxT1hUUfU4txuBp4MiK6d2IZqfun3mNem3p/jhjs\nH2Ur/Mqm3QDcU5zem9LgrwK2UPZZmNLbs0cprTN4EfgaZZ+XSH/mHwosoPQZ+3ngW3z4s2knpbXt\nW4FfUHpA/KCsfjbwbFF/Gfh832UWvT9E6d3MHsCNwI0V+plA4rMefT4rF9OOBP6T0gPmdUqbr6YW\ntQ5KD8AtwFPAP1B5hd8+lJ5011Ja4baE/1+R9ffF9b8JHE/plfwtKq/wa/q4DvBxNw34dXF73wCe\nBP6qrH4RpbflbxTjOLbKfbGz90/NY16vrKm4cjPLzK7wtt/MGsDhN8uUw2+WKYffLFNN/cHByJEj\nY8KECc1cpFlWuru72bRpUy3faxlc+Ivtsj+m9PXRmyPimtTlJ0yYQFdX12AWaWYJnZ2dNV92wG/7\ni29U/RQ4AzgaOK/48oiZ7QIG85l/GvByRKyKiPco/WpsRn3aMrNGG0z4x/DhHy6s4cM/dABA0hxJ\nXZK6enp6BrE4M6unhq/tj4h5EdEZEZ0dHR2NXpyZ1Wgw4V/Lh3+tNZYP/8rJzNrYYML/W+BwSRMl\n7QmcS2kPO2a2Cxjwpr6I+EDSX1Pax94QYH5EPFe3zsysoQa1nT8iHgAeqFMvZtZE/nqvWaYcfrNM\nOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98s\nUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3\ny9SgDtEtqRvYCmwHPoiIzno0ZWaNN6jwF06JiE11uB4zayK/7TfL1GDDH8BDkp6WNKe/C0iaI6lL\nUldPT88gF2dm9TLY8H8mIqYCZwAXSzqx7wUiYl5EdEZEZ0dHxyAXZ2b1MqjwR8Ta4v9G4FfAtHo0\nZWaNN+DwSxomad/e08BpwMp6NWZmjTWYtf2jgF9J6r2e2yPiwbp0ZVaD7du3J+tDhgxpUie7pgGH\nPyJWAVPq2IuZNZE39ZllyuE3y5TDb5Yph98sUw6/Wabq8cMes4b4yU9+kqxfffXVyfq7775bsXbm\nmWcm5507d26yvjt8W9Wv/GaZcvjNMuXwm2XK4TfLlMNvlimH3yxTDr9Zpryd31pm6dKlyfoll1yS\nrJ922mnJ+hFHHFGx9vOf/zw57yOPPJKsn3766cn6xIkTk/VZs2ZVrI0fPz45b734ld8sUw6/WaYc\nfrNMOfxmmXL4zTLl8JtlyuE3y5S381vLrFw5uMM83H777cn6iBEjKta+8Y1vJOedPXt2sr5w4cJk\nPbUvAYCbbrqpYu2JJ55IzjtmzJhkvVZ+5TfLlMNvlimH3yxTDr9Zphx+s0w5/GaZcvjNMrXbbOf/\nwx/+kKzvtddeTerEavX+++8Pav499ki/dq1YsaJi7bvf/W5y3ohI1qs9nqpt51+9enXF2gMPPJCc\n98ILL0zWa1X1lV/SfEkbJa0sm3agpMWSXir+H1CXbsysaWp5238r0He3JZcDD0fE4cDDxXkz24VU\nDX9ELAHe6DN5BnBbcfo24Kw692VmDTbQFX6jImJdcXo9MKrSBSXNkdQlqaunp2eAizOzehv02v4o\nrRmpuHYkIuZFRGdEdO4OBzc0210MNPwbJI0GKP5vrF9LZtYMAw3/IqB338OzgPvq046ZNUvV7fyS\nFgInAyMlrQG+D1wD3CnpK8BrwMx6NFNtW/35559fsVZt2+jMmekWr7jiimQ9tQ94q2zLli0Vazff\nfHNy3iOPPDJZf+edd5L1k046qWJt7733Ts573HHHJeujRlVczQXApZdemqxv3769Ym369OnJeeul\navgj4rwKpVPr3IuZNZG/3muWKYffLFMOv1mmHH6zTDn8Zplqq5/03nXXXcn6vffeO+DrXrBgQbJ+\nxx13JOuTJ0+uWDvkkEOS81arVzsk89ixY5P1cePGVaztt99+yXm3bduWrFezbt26ZP3KK6+sWHvx\nxReT81bbfLt8+fJkffPmzRVrjz32WHLeE088MVnfHfiV3yxTDr9Zphx+s0w5/GaZcvjNMuXwm2XK\n4TfLVFtt5z/33HOT9dTPIK+//vrkvMuWLUvWq+2qOfUTzu7u7uS8S5cuTdZ3592bpb6jcP/99yfn\nPeWUU5L1at8xkFSx9tRTTyXn9XZ+M9ttOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sU6q2fbueOjs7\no6urq2nLK3fjjTcm6xdffHGyfuqplXdWXG0/A8OHD0/Wq+2COnU4Z4A1a9ZUrL3++uvJeffff/9k\nPbWtHGDPPfdM1qdNm1axVm332YN1wgknVKxVu12/+c1v6t1OU3R2dtLV1ZW+cQW/8ptlyuE3y5TD\nb5Yph98sUw6/WaYcfrNMOfxmmWqr3/M30kUXXZSsjxgxIln/0pe+VLE2Y8aM5LyLFy9O1vfZZ59k\nvdrhwX348P6dffbZFWuXX355ct7169cn6wcddNCAemonVV/5Jc2XtFHSyrJpV0laK2lZ8XdmY9s0\ns3qr5W3/rcDp/UyfGxFTi7/0oVXMrO1UDX9ELAHeaEIvZtZEg1nhd4mkZ4uPBQdUupCkOZK6JHXt\nzvuqM9vVDDT8NwCHAlOBdcCPKl0wIuZFRGdEdHZ0dAxwcWZWbwMKf0RsiIjtEbEDuAmo/NMtM2tL\nAwq/pNFlZ88GVla6rJm1p6rb+SUtBE4GRkpaA3wfOFnSVCCAbuBrDeyxKc4555xk/e23365Ymz17\ndnLeoUOHJuuTJk1K1mfOnJmsp76DMH78+OS8u7ItW7Yk60uWLKlYq/Z7/mrrp3aH7fxVwx8R5/Uz\n+ZYG9GJmTeSv95plyuE3y5TDb5Yph98sUw6/Waay2XX3YKXG6Z577knOW+3w4Ndee22y/t577yXr\nKZ/85CeT9cMOOyxZnzhxYsPqY8aMSc579913J+tz585N1rdu3VqxNn/+/OS8F1xwQbLerrzrbjOr\nyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmfJ2/ibYsWNHsj5kyJBk/YorrkjWU4fZfvzxx5PzvvLK\nK8n6qlWrkvVqhxcfjGo/u632M+zvfe97FWuTJ08eUE/tztv5zawqh98sUw6/WaYcfrNMOfxmmXL4\nzTLl8JtlKptDdLfStm3bBjX/okWLkvXjjz++Yu3YY49NznvZZZcl69OnT0/WN2zYkKy/+uqrFWvd\n3d3JeadMmZKsH3XUUcm6pfmV3yxTDr9Zphx+s0w5/GaZcvjNMuXwm2XK4TfLVC2H6B4HLABGUTok\n97yI+LGkA4F/AyZQOkz3zIjY3LhWd13Dhg1L1i+66KJkffny5cn6gw8+WLG2fv365LzPPPNMsn7f\nffcl66NGjRpwPfX9BGu8Wl75PwAujYijgeOBiyUdDVwOPBwRhwMPF+fNbBdRNfwRsS4inilObwVe\nAMYAM4DbiovdBpzVqCbNrP526jO/pAnAMcCTwKiIWFeU1lP6WGBmu4iawy9pOHAP8M2I2FJei9KO\nAPvdGaCkOZK6JHX19PQMqlkzq5+awi/p45SC/8uIuLeYvEHS6KI+GtjY37wRMS8iOiOis6Ojox49\nm1kdVA2/SrtQvQV4ISKuKystAmYVp2cB6dXCZtZWavlJ76eBC4AVknqPNf0d4BrgTklfAV4DZjam\nxV1ftV1Q33DDDQ1bdrXdhlu+qoY/IpYClR69p9a3HTNrFn/DzyxTDr9Zphx+s0w5/GaZcvjNMuXw\nm2XKu+7eze2xh5/frX9+ZJhlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TD\nb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmaoafknj\nJP23pOclPSfpb4vpV0laK2lZ8Xdm49s1s3qp5aAdHwCXRsQzkvYFnpa0uKjNjYhrG9eemTVK1fBH\nxDpgXXF6q6QXgDGNbszMGmunPvNLmgAcAzxZTLpE0rOS5ks6oMI8cyR1Serq6ekZVLNmVj81h1/S\ncOAe4JsRsQW4ATgUmErpncGP+psvIuZFRGdEdHZ0dNShZTOrh5rCL+njlIL/y4i4FyAiNkTE9ojY\nAdwETGtcm2ZWb7Ws7RdwC/BCRFxXNn102cXOBlbWvz0za5Ra1vZ/GrgAWCFpWTHtO8B5kqYCAXQD\nX2tIh2bWELWs7V8KqJ/SA/Vvx8yaxd/wM8uUw2+WKYffLFMOv1mmHH6zTDn8Zply+M0y5fCbZcrh\nN8uUw2+WKYffLFMOv1mmHH6zTDn8ZplSRDRvYVIP8FrZpJHApqY1sHPatbd27Qvc20DVs7dDIqKm\n/eU1NfwfWbjUFRGdLWsgoV17a9e+wL0NVKt689t+s0w5/GaZanX457V4+Snt2lu79gXubaBa0ltL\nP/ObWeu0+pXfzFrE4TfLVEvCL+l0Sb+T9LKky1vRQyWSuiWtKA473tXiXuZL2ihpZdm0AyUtlvRS\n8b/fYyS2qLe2OGx74rDyLR27djvcfdM/80saArwIfA5YA/wWOC8inm9qIxVI6gY6I6LlXwiRdCLw\nFrAgIiYX034IvBER1xRPnAdExGVt0ttVwFutPmx7cTSp0eWHlQfOAr5MC8cu0ddMWjBurXjlnwa8\nHBGrIuI94A5gRgv6aHsRsQR4o8/kGcBtxenbKD14mq5Cb20hItZFxDPF6a1A72HlWzp2ib5aohXh\nHwOsLju/hhYOQD8CeEjS05LmtLqZfoyKiHXF6fXAqFY204+qh21vpj6HlW+bsRvI4e7rzSv8Puoz\nETEVOAO4uHh725ai9JmtnbbV1nTY9mbp57Dyf9TKsRvo4e7rrRXhXwuMKzs/tpjWFiJibfF/I/Ar\n2u/Q4xt6j5Bc/N/Y4n7+qJ0O297fYeVpg7Frp8PdtyL8vwUOlzRR0p7AucCiFvTxEZKGFStikDQM\nOI32O/T4ImBWcXoWcF8Le/mQdjlse6XDytPisWu7w91HRNP/gDMprfF/BbiiFT1U6OtQYHnx91yr\newMWUnob+D6ldSNfAUYADwMvAQ8BB7ZRb78AVgDPUgra6Bb19hlKb+mfBZYVf2e2euwSfbVk3Pz1\nXrNMeYWfWaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5ap/wOCX2rQNj8RsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127d28fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classTypeId = 0 #values between 0..10\n",
    "imageNumber = 12 #value between 0..500\n",
    "with tf.Session() as sess:\n",
    "    # restore the model\n",
    "    tf.train.Saver().restore(sess, savefile)\n",
    "\n",
    "    myclass = sih.fileList[classTypeId]\n",
    "    evalImage = (np.load('./SKETCH_data/{}.npy'.format(myclass))[500 + imageNumber] / 255)\n",
    "\n",
    "    feed_dict = {x: np.reshape(evalImage,newshape=(1,784)), y_true: np.zeros((1, 10)), hold_prob : 0.5 }\n",
    "\n",
    "    classification = sess.run(tf.argmax(y_pred,1), feed_dict)\n",
    "\n",
    "    display(evalImage, myclass, sih.fileList[int(classification)])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def predict(self, classTypeId, imageNumber):\n",
    "    with self.tf.Session() as sess:\n",
    "        # restore the model\n",
    "        self.tf.train.Saver().restore(sess, self.savefile)\n",
    "\n",
    "        myclass = self.sih.fileList[classTypeId]\n",
    "        evalImage = (np.load('./SKETCH_data/{}.npy'.format(myclass))[500 + imageNumber] / 255)\n",
    "\n",
    "        feed_dict = {x: np.reshape(evalImage,newshape=(1,784)), y_true: np.zeros((1, 10)), hold_prob : 0.5 }\n",
    "\n",
    "        classification = sess.run(tf.argmax(y_pred,1), feed_dict)\n",
    "\n",
    "        self.display(evalImage, myclass, sih.fileList[int(classification)])\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'imageNumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-bdebb119c123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassTypeId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimageNumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassTypeId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'imageNumber'"
     ]
    }
   ],
   "source": [
    "classTypeId = 0\n",
    "imageNumber = 1\n",
    "predict(classTypeId, imageNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
